% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/2-2-dnnet.R
\name{dnnet}
\alias{dnnet}
\title{Multilayer Perceptron Model for Regression or Classification}
\usage{
dnnet(train, validate = NULL, load.param = FALSE,
  initial.param = NULL, norm.x = TRUE,
  norm.y = ifelse(is.factor(train@y), FALSE, TRUE), activate = "relu",
  n.hidden = c(10, 10), learning.rate = ifelse(learning.rate.adaptive
  \%in\% c("adam"), 0.001, 0.01), l1.reg = 0, l2.reg = 0,
  n.batch = 100, n.epoch = 100,
  early.stop = ifelse(is.null(validate), FALSE, TRUE),
  early.stop.det = 1000, plot = FALSE, accel = c("rcpp", "none")[1],
  learning.rate.adaptive = c("constant", "adadelta", "adagrad",
  "momentum", "adam", "amsgrad")[5], rho = c(0.9, 0.95, 0.99,
  0.999)[ifelse(learning.rate.adaptive == "momentum", 1, 3)],
  epsilon = c(10^-10, 10^-8, 10^-6, 10^-4)[2], beta1 = 0.9,
  beta2 = 0.999, loss.f = ifelse(is.factor(train@y), "logit", "mse"),
  pathway = FALSE, pathway.list = NULL, pathway.active = "identity",
  l1.pathway = 0, l2.pathway = 0, ...)
}
\arguments{
\item{train}{A \code{dnnetInput} object, the training set.}

\item{validate}{A \code{dnnetInput} object, the validation set, optional.}

\item{norm.x}{A boolean variable indicating whether to normalize the input matrix.}

\item{norm.y}{A boolean variable indicating whether to normalize the response (if continuous).}

\item{activate}{Activation Function. One of the following,
"sigmoid", "tanh", "relu", "prelu", "elu", "celu".}

\item{n.hidden}{A numeric vector for numbers of nodes for all hidden layers.}

\item{learning.rate}{Initial learning rate, 0.001 by default; If "adam" is chosen as
an adaptive learning rate adjustment method, 0.1 by defalut.}

\item{l1.reg}{weight for l1 regularization, optional.}

\item{l2.reg}{weight for l2 regularization, optional.}

\item{n.batch}{Batch size for batch gradient descent.}

\item{n.epoch}{Maximum number of epochs.}

\item{early.stop}{Indicate whether early stop is used (only if there exists a validation set).}

\item{early.stop.det}{Number of epochs of increasing loss to determine the early stop.}

\item{plot}{Indicate whether to plot the loss.}

\item{accel}{"rcpp" to use the Rcpp version and "none" (default) to use the R version for back propagation.}

\item{learning.rate.adaptive}{Adaptive learning rate adjustment methods, one of the following,
"constant", "adadelta", "adagrad", "momentum", "adam".}

\item{epsilon}{A parameter used in Adagrad and Adam.}

\item{beta1}{A parameter used in Adam.}

\item{beta2}{A parameter used in Adam.}

\item{loss.f}{Loss function of choice.}

\item{pathway}{If pathway.}

\item{pathway.list}{Pathway list.}
}
\value{
Returns a \code{DnnModelObj} object.
}
\description{
Fit a Multilayer Perceptron Model for Regression or Classification
}
\seealso{
\code{\link{dnnet-class}}\cr
\code{\link{dnnetInput-class}}\cr
\code{\link{actF}}
}
